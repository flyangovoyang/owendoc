<!doctype html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>模板</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
          integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
</head>
<body>
<div class="container">
    <h1 class="text-center text-primary">torch.utils.data</h1>
    <p class="text-center">
        <a href="index.html"><span class="glyphicon glyphicon-home"></span>返回首页</a>&nbsp;&nbsp;
        <span>create: xxxx</span>
    </p>
    <h3>概览</h3>
		<p><span class="bg-gray">torch.utils.data</span>torch.utils.data主要是负责容纳数据集、数据打散、分批等操作。</p>
		<p>这里面有三个概念：数据集dataset，抽样器sampler，数据加载器dataloader。其中第三个就是最终对外的接口，也是最重要的。</p>
		<p>它们之间的关系是：首先需要根据源数据创建数据集dataset，然后根据dataset创建抽样器sampler，最后同时通过dataset和sampler来创建dataloader，在训练、测试的时候，通过dataloader可以很方便地得到batch数据。</p>
		<h3>dataset</h3>
		<p>第一个是dataset，就是常规理解的数据集。</p>
		<p>数据集主要分为两种：map-style和iterable-style</p>
		<p>map-style数据集，一般都是继承Dataset类，必须要实现<span class="bg-phrase">__getitem__()</span>和<span class="bg-phrase">__len__()</span>方法，表示从索引或者key到数据样本的映射</p>
		<p>iterable-style数据集，一般都是继承IterableDataset类，必须实现<span class="bg-phrase">__iter__()</span>方法，表示在数据样本上迭代。一般从一些流中实时获取数据（比如数据库、远程服务器或者日志），是无法进行随机读取的，这时就主要使用迭代式数据集。</p>
		<blockquote>
			一般如果数据量小，使用map-style就可以了，如果数据量很大，需要从数据流中获取，那就使用iterable-style
		</blockquote>
		<p>对应到具体的类，有以下六个：</p>
		<ul>
			<li>torch.utils.data.Dataset</li>
			<li>torch.utils.data.IterableDataset</li>
			<li>torch.utils.data.TensorDataset</li>
			<li>torch.utils.data.ConcatDataset</li>
			<li>torch.utils.data.ChainDataset</li>
			<li>torch.utils.data.Subset</li>
		</ul>
		<p>除此之外，torch.utils.data还包含了两个函数</p>
		<ul>
			<li>torch.utils.data.get_worker_info()</li>
			<li>torch.utils.data.random_split()</li>
		</ul>
		<h3>sampler</h3>
		<p>sampler是抽样器，作用在dataset上面</p>
		<p>抽样的方式也有几个方式：</p>
		<p>按顺序抽样，随机抽样，在子集合中随机抽样，带权重的抽样等等</p>
		<p>来看下类：</p>
		<ul>
			<li>class Sampler</li>
			<li>class SequentialSampler</li>
			<li>class RandomSampler</li>
			<li>class SubsetRandomSampler</li>
			<li>class WeightedRandomSampler</li>
			<li>class BatchSampler</li>
			<li>class DistributedSampler</li>
		</ul>
		<p>生成sampler的最终目的就是为了创建dataloader，接下来就是torch.utils.data的核心类Dataloader了。</p>
		<h3>dataLoader</h3>
		<pre>DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None)</pre>
		<p>构建DataLoader有几个重要的参数：</p>
		<ul>
			<li>dataset是数据集，</li>
			<li>batch_size是批大小。<mark>有个坑，如果指定了batch_size那么dataloader每次生成[batch_size, dim]，否则默认返回[dim]</mark></li>
			<li>shuffle 是否每一轮都将数据进行打散</li>
			<li>sampler 生成indices，如果设置了sampler那么shuffle必须是False了</li>
			<li>collate_fn</li>
			<li>pin_memory</li>
		</ul>
		<h3>实例1：通过TensorDataset快速生成dataloader</h3>
		<pre>import torch
from torch.utils.data import DataLoader, TensorDataset, Dataset, RandomSampler
import numpy as np


# 创建TensorDataset
feature = torch.tensor(np.arange(100))
dataset = TensorDataset([feature, feature])
sampler = RandomSampler(dataset)
dataloader = DataLoader(dataset, batch_size=5, sampler=sampler)

for epoch in range(2):
    print('epoch=', epoch)
    for index, batch in enumerate(dataloader):
        print(batch)
        if index > 10:
            break
</pre>
		<pre>epoch= 0
[tensor([79,  6, 81, 35, 21], dtype=torch.int32), tensor([79,  6, 81, 35, 21], dtype=torch.int32)]
[tensor([43, 98, 86, 23, 68], dtype=torch.int32), tensor([43, 98, 86, 23, 68], dtype=torch.int32)]
[tensor([ 0, 36, 60,  1, 91], dtype=torch.int32), tensor([ 0, 36, 60,  1, 91], dtype=torch.int32)]
[tensor([71, 59, 72, 75, 52], dtype=torch.int32), tensor([71, 59, 72, 75, 52], dtype=torch.int32)]
[tensor([45,  2, 73, 46, 95], dtype=torch.int32), tensor([45,  2, 73, 46, 95], dtype=torch.int32)]
[tensor([82, 37, 24, 12, 16], dtype=torch.int32), tensor([82, 37, 24, 12, 16], dtype=torch.int32)]
[tensor([90, 11, 70, 31, 53], dtype=torch.int32), tensor([90, 11, 70, 31, 53], dtype=torch.int32)]
[tensor([15,  7, 64, 22, 65], dtype=torch.int32), tensor([15,  7, 64, 22, 65], dtype=torch.int32)]
[tensor([ 3, 87,  4, 17, 99], dtype=torch.int32), tensor([ 3, 87,  4, 17, 99], dtype=torch.int32)]
[tensor([83, 20, 19, 89, 42], dtype=torch.int32), tensor([83, 20, 19, 89, 42], dtype=torch.int32)]
[tensor([97, 58,  8, 38, 30], dtype=torch.int32), tensor([97, 58,  8, 38, 30], dtype=torch.int32)]
[tensor([54, 56, 48, 27, 57], dtype=torch.int32), tensor([54, 56, 48, 27, 57], dtype=torch.int32)]
epoch= 1
[tensor([66, 15, 37, 82, 47], dtype=torch.int32), tensor([66, 15, 37, 82, 47], dtype=torch.int32)]
[tensor([75, 70,  5, 99, 33], dtype=torch.int32), tensor([75, 70,  5, 99, 33], dtype=torch.int32)]
[tensor([80, 76, 55, 29, 41], dtype=torch.int32), tensor([80, 76, 55, 29, 41], dtype=torch.int32)]
[tensor([79, 17, 63, 92, 74], dtype=torch.int32), tensor([79, 17, 63, 92, 74], dtype=torch.int32)]
[tensor([52, 53, 58, 38, 87], dtype=torch.int32), tensor([52, 53, 58, 38, 87], dtype=torch.int32)]
[tensor([84, 59, 77, 48, 71], dtype=torch.int32), tensor([84, 59, 77, 48, 71], dtype=torch.int32)]
[tensor([56, 16, 27, 81, 60], dtype=torch.int32), tensor([56, 16, 27, 81, 60], dtype=torch.int32)]
[tensor([50, 73, 46, 28, 32], dtype=torch.int32), tensor([50, 73, 46, 28, 32], dtype=torch.int32)]
[tensor([45, 40, 10, 25,  9], dtype=torch.int32), tensor([45, 40, 10, 25,  9], dtype=torch.int32)]
[tensor([12, 49, 22, 51, 20], dtype=torch.int32), tensor([12, 49, 22, 51, 20], dtype=torch.int32)]
[tensor([ 6, 68, 72, 24, 67], dtype=torch.int32), tensor([ 6, 68, 72, 24, 67], dtype=torch.int32)]
[tensor([57, 96, 23, 97, 98], dtype=torch.int32), tensor([57, 96, 23, 97, 98], dtype=torch.int32)]
```</pre>
		<p>使用TensorDataset+RandomSampler既可以实现打散，也可以实现分批。</p>
		<h3>实例2：自定义Dataset</h3>
		<pre>python
import torch
import torch.nn as nn
import numpy as np
import random
from torch.utils.data import Dataset, DataLoader

class ToyDataset(Dataset):
    def __init__(self):
        self.Data = np.arange(32).reshape(16, 2).tolist()
        self.Target = np.random.randint(0, 2, (16,1)).tolist()

    def __getitem__(self, index):
        txt = torch.LongTensor(self.Data[index])
        label = torch.LongTensor(self.Target[index])
        return txt, label

    def __len__(self):
        return len(self.Data)
</pre>
</div>
<script src="https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
        integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
        crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
        integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
        crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters:
                [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ]
        });
    });
</script>
</script>
</body>
</html>