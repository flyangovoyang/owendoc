<!doctype html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>模板</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
          integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
</head>
<body>
<div class="container">
    <h1 class="text-center text-primary">torch</h1>
    <p class="text-center">
        <span class="glyphicon glyphicon-home"></span> <a href="index.html">返回首页</a>
        <span class="glyphicon glyphicon-time"></span> 2020-01-06
    </p>
    <hr>
    <ul>
        <li><code>torch.is_tensor(obj)</code> 如果obj是个pytorch张量返回True</li>
        <li><code>torch.is_storage(obj)</code></li>
        <li><code>torch.is_complex(obj)</code></li>
        <li><code>torch.is_floating_point(input)</code></li>
        <li><code>torch.set_default_dtype(d)</code> 设置默认的浮点类型，比如设置为torch.float64而不是torch.float32</li>
        <li><code>torch.get_default_dtype()</code> 返回当前默认的浮点类型</li>
        <li><code>torch.set_default_tensor_type(t)</code> 设置默认的张量的数据类型为t，比如t为torch.DoubleTensor</li>
        <li class="bold text-primary"><code>torch.numel(input)</code> 返回input张量中的元素总数</li>
        <li><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)</code> 设置张量的打印格式</li>
        <li><code>torch.set_flush_denormal(mode)</code> ???</li>
    </ul>
    <h3 class="text-primary">创建</h3>
    <ul>
        <li><code>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)</code></li>
        <li><code>torch.sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False)</code>以坐标的格式构建一个稀疏张量，非零元素通过indices和values来指定。稀疏张量是不可恢复的，即如果indices中存在重复坐标，索引处的值将是所有重复值的和。</li>
        <li><code>torch.as_tensor(data, dtype=None, device=None)</code> 将data转成torch.Tensor，如果data已经是有相同dtype和device的张量，那么不会执行复制；否则如果张量的require_grad=True那么会返回一个保留了计算图的新的张量。类似地，如果数据是对应dtype的ndarray并且device是cpu，也不会执行任何复制。</li>
        <li><code>torch.as_strided(input, size, stride, storage_offset=0)</code> 给当前存在的带有size、stride和storage_offset的input张量创建一个视图。</li>
        <li class="bold"><code>torch.from_numpy(ndarray)</code> 从numpy导入</li>
        <li><code>torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
        <li><code>torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)</code></li>
        <blockquote>类似地还有torch.ones()、torch.ones_like()、torch.full()、torch.full_like()</blockquote>
        <li><code>torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code> range()已弃用</li>
        <li><code>torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code> 范围内指定个数来生成</li>
        <li><code>torch.logspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
        <li><code>torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
        <li><code>torch.empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code> 随机值</li>
        <li><code>torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.perserve_format)</code></li>
        <li><code>torch.empty_strided(size, stride, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False)</code></li>
        <li><code>torch.quantize_per_tensor(input, scale, zero_point, dtype)</code> Converts a float tensor to quantized tensor with given scale and zero point？？？</li>
        <li><code>torch.quantize_per_channel(input, scales, zero_inputs, axis, dtype)</code> Converts a float tensor to per-channel quantized tensor with given scales and zero points.</li>
    </ul>
    <h3 class="text-primary">索引、切片、连接和可变操作</h3>
    <ul>
        <li><code>torch.cat(tensors, dim=0, out=None)</code> 可以看成是torch.split()和torch.chunk()的逆操作</li>
        <li><code>torch.chunk(input, chunks, dim=0)</code> 将一个张量分成指定数量的块，每个块都是input张量的一个视图。</li>
        <li><code>torch.gather(input, dim, index, out=None, sparse_grad=False)</code></li>
        <li><code>torch.index_select(input, dim, index, out=None)</code> returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor</li>
        <li><code>torch.masked_select(input, mask, out=None)</code></li>
        <li><code>torch.narrow(input, dim, start, length)</code></li>
        <li><code>torch.nonzero(input, *, out=None, as_tuple=False)</code></li>
        <li><code>torch.reshape(input, shape)</code></li>
        <li><code>torch.split(tensor, split_size_or_sections, dim=0)</code></li>
        <li><code>torch.squeeze(input, dim=None, out=None)</code></li>
        <li><code>torch.stack(tensors, dim=0, out=None)</code></li>
        <li><code>torch.t(input)</code></cod></li>
        <li><code>torch.take(input, index)</code></li>
        <li><code>torch.transpose(input, dim0, dim1)</code></li>
        <li><code>torch.unbind(input, dim=0)</code> 移除张量一个轴，解绑，这样的话就变成若干个张量</li>
        <li><code>torch.unsqueeze(input, dim)</code> </li>
        <li><code>torch.where()</code> 根据判断条件返回布尔张量</li>
    </ul>
    <h3 class="text-primary">生成器</h3>
    <ul>
        <li><code>torch._C.Generator(device='cpu')</code> 返回一个generator</li>
    </ul>
    <h3 class="text-primary">随机采样</h3>
    <ul>
        <li><code>torch.seed()</code></li>
        <li><code>torch.manual_seed(seed)</code></li>
        <li><code>torch.initial_seed()</code></li>
        <li><code>torch.get_rng_state()</code></li>
        <li><code>torch.set_rng_state(new_state)</code></li>
        <li><code>torch.default_generator</code></li>
        <li><code>torch.bernoulli(input, *, generator=None, out=None)</code></li>
        <li><code>torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None)</code></li>
        <li><code>torch.normal()</code> 有两种<code>torch.normal(mean, std, *, generator=None, out=None)</code>、<code>torch.normal(mean=0.0, std, out=None)</code></li>
        <li><code>torch.poisson(input *, generator=None)</code></li>
        <li><code>torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
        <li><code>torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)</code></li>
        <li><code>torch.randint(low=0, high, size, generator=None, out=None)</code></li>
        <li><code>torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format)</code></li>
        <li><code>torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)</code></li>
        <li><code>torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)</code></li>
        <li><code>torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False)</code> 返回一个随机的从0到n-1的全排列</li>
    </ul>
    <h3 class="text-primary">原地随机采样</h3>
    <ul>
        <li><code>torch.Tensor.bernouli()</code></li>
        <li><code>torch.Tensor.cauchy_()</code></li>
        <li><code>torch.Tensor.exponential_()</code></li>
        <li><code>torch.Tensor.geometric_()</code></li>
        <li><code>torch.Tensor.log_normal_()</code></li>
        <li><code>torch.Tensor.normal_()</code></li>
        <li><code>torch.Tensor.random_()</code></li>
        <li><code>torch.Tensor.uniform_()</code></li>
    </ul>
    <h3 class="text-primary">准随机采样Quasi-random sampling</h3>
    <ul>
        <li><code>torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)</code></li>
    </ul>
    <h3 class="text-primary">序列化</h3>
    <ul>
        <li><code>torch.save(obj, f, pickle_module=&lt;module 'pickle' from 'opt/conda/lib/python3.6/pickle.py'&gt;, pickle_protocol=2, _use_new_zipfile_serializatioin=False)</code></li>
        <li><code>torch.load(f, map_location=None, pickle_module=&lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&gt;, **pickle_load_args)</code></li>
    </ul>
    <h3 class="text-primary">并行化</h3>
    <ul>
        <li><code>torch.get_num_threads()</code></li>
        <li><code>torch.set_num_threads()</code></li>
        <li><code>torch.get_num_interop_threads()</code></li>
        <li><code>torch.set_num_interop_threads()</code></li>
    </ul>
</div>
<style>.bold{font-weight: bold}blockquote{font-size:14px}</style>
<script src="https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
        integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
        crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
        integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
        crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters:
                [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ]
        });
    });
</script>
</script>
</body>
</html>