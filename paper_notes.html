<!doctype html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>模板</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
          integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
</head>
<body>
<div class="container">
    <h1 class="text-center text-primary">Efficient Transformers: A Survey</h1>
    <h3 class="text-center text-primary">Google, 2020-09-16</h3>
    <br>
    <p class="text-center">
        <a href="index.html"><span class="glyphicon glyphicon-home"></span> 返回首页</a>&nbsp;&nbsp;&nbsp;&nbsp;
        <span>create: 2020-12-24</span>
    </p>
    <hr>
    <h3 class="text-primary">摘要</h3>
    <p>一个Transformers的综述文章，提到了差不多17个基于Transformer的改进模型。</p>
    <h3 class="text-primary">主要内容</h3>
    <p>有几个关于Transformer的描述可以借鉴下：</p>
    <ol>
        <li>The self-attention mechanism is a key defining characteristic of Transformer models.</li>
        <li>A well-known concern with self-attention is the quadratic time and memory complexity.</li>
        <li>Transformer blocks are characterized by a multi-head self-attention mechanism, a position-wise feed-forward network, layer normalization modules and residual connectors.</li>
        <li>Positional encodings can take the form of a sinusoidal input or be trainable embeddings.</li>
        <li>It is important to note the differences in the mode of usage of the Transformer block. Transformers can primarily be used in three ways, namely: (1)encoder-only, e.g. for classification; (2)decoder-only, e.g. for language modeling; (3)encoder-decoder, e.g. for machine translation.</li>
    </ol>
    <p>文章对这17个模型改进做了分类，大致可分为五大类。</p>
    <h3 class="text-primary">结论</h3>
    <p>最后，比较重要的结论是</p>
    <blockquote>It's a still a mystery to which fundamental efficient Transformer block one should consider using.</blockquote>
    <p>没错，到底谁好谁不好，文章表示不同的模型彼此之间的评测标准不一样，有些还涉及到了预训练，所以，孰优孰劣无法得出结论。这让我觉得这篇文章也是在灌水。</p>
</div>
<script src="https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
        integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
        crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
        integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
        crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters:
                [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ]
        });
    });
</script>
</script>
</body>
</html>